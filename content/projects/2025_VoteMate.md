---
title: "Consulting Project on Applications of AI in Voting"
date: 2025-12-10
season: Fall 2025
draft: false
language: en
featured_image: ../assets/images/featured/Projects_2025_12_VoteMate.png
summary: Worked alongside four teammates as a consultant for VoteMate in CMU's Policy Innovation Lab, applying an agile framework as we explored the intersection of artificial intelligence and elections.
author: Carnegie Mellon University
location: City of Pittsburgh, PA, USA
authorimage: ../assets/images/global/logos/logo_cmu_seal.png
# categories: Featured
---

### Summary

**Fall 2025 Consulting Project through the CMU Policy Innovation Lab**

Conducted by Danielle Aira Savellano, Belen Torres, Alejandro Ortiz Lopez, Nishan Sah, and Brett Nyman

This project focused on improving the Data Pipeline and AI Agent for [VoteMate](https://www.votemateus.org/), a non-profit civic assistant, to provide accurate, accessible, and nonpartisan election information. The core challenge addressed was the difficulty of reliably aggregating information for low-visibility races due to inconsistent primary sources (like candidate websites) and a significant lack of structured data.

This work was conducted as a final project and **successfully handed off to VoteMate**. To respect the proprietary nature of the final solution, specific technical specifications and ongoing management details are now the responsibility of the partner and are not publicly disclosed.

*Involved user research, partnership proposal writing and design, and prototyping for pre-scraping analysis and an interactive scraper.*

### Project Management Approach

This project was executed using an Agile framework to ensure flexibility, manage complexity, and ensure continuous delivery.

- **User Story Ownership**: Each team member owned specific user stories to clearly define functional requirements, scope individual responsibilities, and ensure comprehensive coverage of the system.
- **Weekly Sprints and Retrospectives**: To manage workflow, adapt to new research findings, and continuously improve processes.
- **Daily Standups (YTBG: Yesterday, Today, Blockers, Goals)**: To maintain alignment, discuss progress, and quickly resolve blockers.

Effective communication was maintained across all stakeholders, including weekly progress reports delivered to the professor and teaching assistant, formal updates to the partner organization (VoteMate), and clear communication with interviewees. Documentation practices (ex. partner README file) ensured a clear record of research, design decisions, and technical specifications for a successful hand-off.

### Pivoting from Direct Scraping to a Partnership Model

Initially, the team attempted to create a general scraper to ingest information from all candidate websites. However, user research and secondary analysis highlighted several critical issues that led to a pivot:

- **Inconsistent Website Designs**: Candidate websites use different CMS platforms (e.g., WordPress, Squarespace), rendering techniques (HTML vs. JavaScript), and lack a standardized schema.
- **Prohibitive Effort**: Accounting for the variance across websites and errors proved too time-consuming.
- **Ethical Concerns & Duplication**: Interviewees expressed concerns about the trustworthiness of AI and noted that extensive scraping would duplicate efforts already being conducted by Voter Engagement organizations.

The proposed strategy is to leverage existing efforts through partnerships.

### Research Objectives

Our group aimed to improve the reliability and scale of election data by:

- Conducting **user research and secondary research** to identify sources for verified, structured data, thereby strengthening the civic information ecosystem and ensuring ethical data practices.
- Designing a **partnership package** to showcase information about VoteMate, the future of AI in the election space, partnership motivations and benefits/responsibilities, and the ethical implications regarding VoteMate's use of AI to build trust with partners and users.
- Developing a **pre-scraping analysis framework and Interactive scraper** to address remaining data gaps efficiently by profiling websites before extraction.

### Key Finding

We found that the most effective strategy to ensure data accuracy and build trust in an AI civic assistant is through a **combination of strategic partnerships** with established nonpartisan organizations **and a structured, targeted scraping process**. This hybrid model addresses the unreliability of general scraping while filling coverage gaps for local races, which are often overlooked by major platforms.
